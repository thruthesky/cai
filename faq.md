# JAI FAQ (자주 묻는 질문)

JAI(Job AI) 프로젝트의 핵심 개념을 쉽게 설명합니다.

---

## 목차

1. [JAI 데이터 흐름](#0-jai-데이터-흐름)
2. [구인 정보 LLM이 필요한가?](#1-구인-정보-llm이-필요한가)
3. [샘플 데이터는 실제 구인 정보여야 하는가?](#2-샘플-데이터는-실제-구인-정보여야-하는가)
4. [왜 토큰화를 해야 하는가?](#3-왜-토큰화를-해야-하는가)
5. [토큰화 다음에는 무엇을 해야 하는가?](#4-토큰화-다음에는-무엇을-해야-하는가)

---

## 0. JAI 데이터 흐름

JAI는 **"텍스트 → 숫자로 변환 → AI 훈련 → 텍스트 생성"** 과정입니다.

```
raw.txt → prepare_samples.py → samples.txt
                                    ↓
                            train_tokenizer.py → tokenizer.json
                                    ↓
                            build_bin_dataset.py → train.bin / val.bin
                                    ↓
                            train_gpt.py → ckpt.pt
                                    ↓
                            generate.py → "서울에서 React 개발자를..."
```

| 단계 | 스크립트 | 하는 일 |
|------|----------|---------|
| 1 | prepare_samples.py | 원본 텍스트를 학습 형식으로 정리 |
| 2 | train_tokenizer.py | 텍스트를 숫자로 바꾸는 규칙 만들기 |
| 3 | build_bin_dataset.py | 토큰 ID를 바이너리 파일로 저장 |
| 4 | train_gpt.py | GPT 모델 학습 |
| 5 | generate.py | 텍스트 생성 |

---

## 1. 구인 정보 LLM이 필요한가?

### 결론: 실용적으로는 "아니오", 학습 목적으로는 "예"

**왜 실용적으로 어려운가?**

구인 정보는 매일 바뀝니다. LLM에 학습된 정보는 고정되어 있어서 금방 쓸모없어집니다.

```
월요일: "카카오에서 채용 중!" 학습
화요일: 채용 마감
수요일: 사용자 질문 → AI가 틀린 정보 제공
```

**실제 서비스는 어떻게?**

RAG(검색 + 생성) 방식을 사용합니다:
1. 최신 구인 DB에서 검색
2. 검색 결과를 LLM에게 전달
3. LLM이 정리해서 답변

**단, 자동화 시스템이 있다면 가능합니다**

매일 새로운 구인 정보를 학습 데이터로 업데이트하고, LLM을 자동으로 재학습시키는 파이프라인(GPU 서버 + CI/CD)을 구축하면 구인 정보 LLM도 실용적으로 운영할 수 있습니다. 특히 데이터 양이 적은 경우(특정 지역/직종만 다루는 경우) 학습 시간이 짧아 매일 재학습이 현실적입니다.

**JAI의 목적**

나만의 인공지능 개발을 처음부터 끝까지 직접 경험하는 것입니다. 자동차 엔진을 직접 조립해보면 자동차 작동 원리를 깊이 이해하게 되는 것과 같습니다.

단, JAI는 단순한 학습용 프로젝트가 아닙니다. 2026년 2월의 최신 구인 정보를 바탕으로 학습하므로, 실제로 사용할 수 있는 구인 정보 AI입니다.

---

## 2. 샘플 데이터는 실제 구인 정보여야 하는가?

### 결론: 예, 실제 구인 정보여야 합니다.

JAI는 학습용으로 개발하는 LLM이지만, 2026년 2월 기준 실제 구인 정보를 제공하는 것이 목표입니다. 따라서 샘플 데이터에도 2026년 2월의 최신 구인 정보를 담아야 합니다.

**핵심 원리: LLM은 "본 것"만 흉내냅니다**

일본어만 듣고 자란 사람이 한국어를 할 수 없듯이, LLM도 학습 데이터에서 본 패턴만 출력합니다.

```
구인 공고로 학습 → 구인 공고 형태로 출력
뉴스 기사로 학습 → 뉴스 기사 형태로 출력
```

**개발 단계별 전략**

| 단계 | 데이터 | 목적 |
|------|--------|------|
| 초기 | 가짜/작은 샘플 | 파이프라인이 돌아가는지 확인 |
| 중간 | 구인 형식의 합성 데이터 | 출력 형식 검증 |
| 최종 | 실제 구인 데이터 | 서비스 품질 달성 |

**권장 학습 데이터 형식**

```
[QUESTION]
서울에서 React 개발자 채용 있어?
[/QUESTION]

[DOC]
ABC Tech에서 프론트엔드 개발자를 채용합니다.
- 위치: 서울 강남구 (하이브리드)
- 자격: React 3년 이상, TypeScript 필수
- 연봉: 6,000~8,000만원
- 마감: 2024-02-15
[/DOC]

[ANSWER]
요약:
- 강남구 스타트업에서 채용 중

구인 정보:
- 회사: ABC Tech
- 포지션: Frontend Developer
- 연봉: 6,000만원 ~ 8,000만원
[/ANSWER]
```

- `[DOC]`가 없으면: 모델이 답변을 "암기"
- `[DOC]`가 있으면: 모델이 "문서를 참고해서 답변하는 방법"을 학습 (RAG 연동에 유리)

모델은 이 형식을 수천 번 보면서 "질문 → 문서 참고 → 답변" 순서를 학습합니다.

---

## 3. 왜 토큰화를 해야 하는가?

### 결론: 컴퓨터는 글자를 이해 못하고 숫자만 이해합니다.

**토큰화란?**

텍스트를 작은 조각(토큰)으로 나누고, 각 조각에 숫자(ID)를 붙이는 것입니다.

```
"서울에서 React 채용"
    ↓ 토큰으로 나누기
["서울", "에서", " ", "React", " ", "채용"]
    ↓ ID 부여
[1523, 892, 3, 4521, 3, 1876]
```

**왜 필요한가?**

1. **GPT 학습 방식**: "다음 토큰 맞추기" 게임으로 학습합니다. 맞추려면 숫자여야 확률 계산이 됩니다.

2. **임베딩 입력**: 모델 내부에서 토큰 ID → 벡터 변환이 일어납니다. 토큰화 없이는 모델에 입력할 수 없습니다.

3. **효율성**: 도메인 특화 토크나이저는 "USD 150k"를 3개 토큰으로, 범용은 7개로 쪼갭니다. 토큰이 적으면 계산도 빠르고 의미도 보존됩니다.

---

## 4. 토큰화 다음에는 무엇을 해야 하는가?

### 순서: 토크나이저 학습 → 바이너리 변환 → GPT 학습 → 생성 테스트

---

### 단계 1: 토크나이저 학습

```bash
uv run python scripts/train_tokenizer.py
```

samples.txt를 분석해서 "어떻게 쪼개면 효율적인지" 규칙(BPE)을 만듭니다.

**결과물:** `tokenizer.json` (어휘 사전 + 병합 규칙)

---

### 단계 2: 바이너리 데이터셋 생성

```bash
uv run python scripts/build_bin_dataset.py
```

텍스트를 토큰 ID로 변환하고, 빠르게 읽을 수 있는 바이너리로 저장합니다.

**학습 데이터 구조:**

```
토큰: [12, 45, 98, 23, 67, 89, ...]
         ↓ block_size=4로 자르기
입력 X: [12, 45, 98, 23]
정답 Y: [45, 98, 23, 67]  ← 한 칸 시프트
```

GPT는 "12 다음엔 45, 45 다음엔 98..." 이런 식으로 다음 토큰을 예측하며 학습합니다.

**결과물:** `train.bin`, `val.bin`

---

### 단계 3: GPT 학습

```bash
uv run python scripts/train_gpt.py
```

**학습 과정:**
1. 토큰 ID → 임베딩(벡터 변환)
2. Transformer 블록 통과 (Self-Attention)
3. 다음 토큰 확률 출력
4. 정답과 비교해서 손실 계산
5. 가중치 업데이트
6. 반복

**결과물:** `checkpoints/ckpt.pt`

---

### 단계 4: 생성 테스트

```bash
uv run python scripts/generate.py
```

학습된 모델로 텍스트를 생성해봅니다. 잘 학습됐다면 구인 정보 형태로 출력합니다.

---

### 하이퍼파라미터 (M4 기준)

| 파라미터 | 값 | 의미 |
|----------|-----|------|
| vocab_size | 24,000 | 어휘 크기 |
| block_size | 256 | 한 번에 처리하는 토큰 수 |
| n_layer | 6 | Transformer 블록 수 |
| n_head | 6 | Attention 헤드 수 |
| n_embd | 384 | 임베딩 차원 |
| batch_size | 16 | 배치 크기 |
| learning_rate | 3e-4 | 학습률 |

---

## 5. 추가 질문

**Q: 임베딩을 별도로 해야 하나요?**

아니요. GPT 모델 내부에 임베딩 레이어가 포함되어 있어서 자동으로 처리됩니다.

**Q: 학습이 잘 되는지 어떻게 알 수 있나요?**

loss 값이 점점 낮아지면 잘 되고 있는 것입니다. train loss는 줄어드는데 val loss가 올라가면 과적합입니다.

**Q: 데이터가 커지면?**

samples.txt를 여러 파일로 나누고 스크립트를 수정하면 됩니다.

---

## 참고 문서

- [모델 구조](docs/05-model-architecture.md)
- [학습 코드](docs/06-training.md)
- [핵심 개념](docs/08-concepts.md)
- [트러블슈팅](docs/09-tips.md)
