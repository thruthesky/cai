# vocab_size란 무엇인가?

## 한 줄 요약

**vocab_size**는 토크나이저가 알고 있는 **단어(토큰)의 총 개수**입니다.

---

## 쉬운 비유

vocab_size는 **사전의 단어 수**와 같습니다.

| 비유 | 설명 |
|------|------|
| 영어 사전 | 약 17만 단어 수록 |
| 초등학생 어휘 | 약 5,000~10,000 단어 |
| **JAI 토크나이저** | **24,000 토큰** |

사전에 없는 단어는 "모르는 단어([UNK])"로 처리됩니다.

---

## 동작 방식

```
vocab_size = 24000 이면:

토큰 ID 범위: 0 ~ 23,999

예시:
- ID 0: [PAD] (패딩)
- ID 1: [UNK] (미지의 토큰)
- ID 2: [BOS] (문장 시작)
- ID 3: [EOS] (문장 끝)
- ID 4: "의"
- ID 5: "을"
- ...
- ID 1523: "서울"
- ID 4521: "React"
- ...
- ID 23999: (마지막 토큰)
```

---

## vocab_size가 중요한 이유

### 1. 모델 크기에 영향

```python
# 임베딩 레이어 파라미터 수
embedding_params = vocab_size × n_embd

# 예시: vocab_size=24000, n_embd=384
24000 × 384 = 9,216,000 파라미터 (약 9.2M)
```

vocab_size가 크면 모델도 커집니다.

### 2. 토큰화 품질에 영향

| vocab_size | 결과 |
|------------|------|
| **너무 작음** (8,000) | 한국어가 글자 단위로 쪼개짐 → 의미 손실 |
| **너무 큼** (100,000) | 희소 토큰 많음 → 학습 어려움 |
| **적절함** (16,000~32,000) | 균형 잡힌 토큰화 |

### 3. 토큰화 예시 비교

**vocab_size = 8,000 (너무 작음)**
```
"안녕하세요" → ["안", "녕", "하", "세", "요"]  (5개 토큰)
```

**vocab_size = 24,000 (적절함)**
```
"안녕하세요" → ["안녕", "하세요"]  (2개 토큰)
```

토큰이 적을수록:
- 모델이 더 긴 문맥을 볼 수 있음
- 의미 단위가 보존됨
- 학습이 효율적

---

## JAI 기본 설정

```python
# scripts/train_tokenizer.py
VOCAB_SIZE = 24000  # 어휘 크기 (16,000 ~ 32,000 권장)
```

| 파라미터 | 값 | 설명 |
|----------|-----|------|
| vocab_size | 24,000 | 토크나이저 어휘 크기 |
| 권장 범위 | 16,000 ~ 32,000 | 한국어 + 영어 혼합 데이터 |

---

## vocab_size 조정이 필요한 경우

### vocab_size 늘려야 할 때

- 한국어가 글자 단위로 쪼개질 때
- [UNK] 토큰이 자주 나올 때
- 다양한 전문 용어가 있을 때

```python
VOCAB_SIZE = 32000  # 24000 → 32000으로 증가
```

### vocab_size 줄여야 할 때

- 데이터가 매우 적을 때 (학습 샘플 부족)
- 메모리가 부족할 때
- 빠른 프로토타이핑이 필요할 때

```python
VOCAB_SIZE = 16000  # 24000 → 16000으로 감소
```

---

## vocab_size vs 다른 파라미터

| 파라미터 | 역할 | JAI 기본값 |
|----------|------|-----------|
| **vocab_size** | 어휘 크기 (토큰 종류 수) | 24,000 |
| block_size | 한 번에 처리하는 토큰 수 | 256 |
| n_embd | 임베딩 벡터 차원 | 384 |
| n_layer | Transformer 블록 수 | 6 |
| n_head | Attention 헤드 수 | 6 |

---

## 자주 하는 실수

### 1. 토크나이저와 모델의 vocab_size 불일치

```python
# ❌ 잘못된 예
# train_tokenizer.py: VOCAB_SIZE = 24000
# train_gpt.py: vocab_size = 32000  # 불일치!

# ✅ 올바른 예
# 둘 다 같은 값 사용
VOCAB_SIZE = 24000
```

### 2. 토크나이저 재학습 후 모델 재학습 안 함

토크나이저를 재학습하면 **모델도 처음부터 다시 학습**해야 합니다.
(토큰 ID가 바뀌기 때문)

---

## 요약

| 질문 | 답변 |
|------|------|
| vocab_size란? | 토크나이저가 아는 토큰의 총 개수 |
| JAI 기본값 | 24,000 |
| 권장 범위 | 16,000 ~ 32,000 |
| 너무 작으면? | 한국어가 글자 단위로 쪼개짐 |
| 너무 크면? | 희소 토큰 많아 학습 어려움 |

---

## 관련 문서

- [왜 토큰화를 해야 하는가?](why-tokenize.md)
- [토큰화 방법](how-to-tokenize.md)
- [핵심 개념 - 토크나이저](core-concepts.md#1-tokenizer-토크나이저)
- [트러블슈팅 - 토크나이저 문제](troubleshooting.md#토크나이저-문제)
